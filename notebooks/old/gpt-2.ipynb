{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DIjF95a6aQ5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665260353095,"user_tz":180,"elapsed":69255,"user":{"displayName":"Matheus Vanzan","userId":"00967685822501260988"}},"outputId":"cad0269f-e818-425c-82c4-882e8281034a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 143 kB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 55.9 MB 288 kB/s \n","\u001b[K     |████████████████████████████████| 717 kB 69.0 MB/s \n","\u001b[K     |████████████████████████████████| 163 kB 66.4 MB/s \n","\u001b[K     |████████████████████████████████| 7.6 MB 61.0 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n","\u001b[K     |████████████████████████████████| 120 kB 62.4 MB/s \n","\u001b[K     |████████████████████████████████| 99 kB 10.1 MB/s \n","\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Sat Oct  8 20:19:10 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["####################################\n","#\n","#  ADD THIS TO EVERY COLAB FILE!\n","#\n","####################################\n","\n","#!pip install -q import-ipynb\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#import import_ipynb\n","import drive.Shareddrives.GPTJ.project.settings as settings\n","\n","PATH_PROJECT = settings.PATH_PROJECT\n","PATH_DATA = settings.PATH_DATA\n","\n","! cd $PATH_PROJECT && pip install -q -r requirements.txt\n","\n","!nvidia-smi"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, random_split\n","from transformers import GPT2Tokenizer, GPT2TokenizerFast, TrainingArguments, Trainer, GPT2LMHeadModel, GPTNeoForCausalLM"],"metadata":{"id":"0Rl0u8CQj0SI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MODEL\n","\n","#model_name = 'EleutherAI/gpt-neo-1.3B' # CUDA out of memory.\n","\n","model_name = 'gpt2' # GPU vRAM 3495MiB - 3.5G\n","#model_name = 'gpt2-medium' # GPU vRAM 7389MiB - 7.3G\n","#model_name = 'gpt2-large' # GPU vRAM 14797MiB - 14.8G\n","\n","#model_name = 'gpt2-xl' # CUDA out of memory.\n"],"metadata":{"id":"axoAqEwsBP-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the random seed to a fixed value to get reproducible results \n","torch.manual_seed(42)\n","\n","# download\n","# tokenizer = GPT2Tokenizer.from_pretrained(\n","tokenizer = GPT2TokenizerFast.from_pretrained(\n","    model_name, \n","    bos_token='<|startoftext|>',\n","    eos_token='<|endoftext|>', \n","    pad_token='<|pad|>'\n",")\n","model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# torch.save(tokenizer, os.path.join(PATH_DATA, model_name, 'tokenizer'))\n","# torch.save(model, os.path.join(PATH_DATA, model_name, 'model'))\n","\n","# load\n","# model = torch.load(os.path.join(PATH_DATA, model_name, 'model'))\n","# tokenizer = torch.load(os.path.join(PATH_DATA, model_name, 'tokenizer'))"],"metadata":{"id":"AR0j06rPj6PU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["descriptions = pd.read_csv(PATH_PROJECT + 'netflix_titles.csv')['description']\n","descriptions = descriptions[:100]\n","\n","max_length = max([len(tokenizer.encode(description)) for description in descriptions])\n","max_length, descriptions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kobrb3SvsJn7","executionInfo":{"status":"ok","timestamp":1665260786127,"user_tz":180,"elapsed":225,"user":{"displayName":"Matheus Vanzan","userId":"00967685822501260988"}},"outputId":"82cdba4c-cc9b-49dd-947a-856751b75240"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(56, 0                           As her father nears the end.\n"," 1      As her father nears the end of his life, filmm...\n"," 2      After crossing paths at a party, a Cape Town t...\n"," 3      To protect his family from a powerful drug lor...\n"," 4      Feuds, flirtations and toilet talk go down amo...\n","                              ...                        \n"," 995    Abandoned by her family, young single mother A...\n"," 996    In 1974, a rural town in Anatolia gets its fir...\n"," 997    Truth and illusion blurs when a homeless amnes...\n"," 998    Using innovative technology, this docuseries e...\n"," 999    Journalists and fans await Ma Anand Sheela as ...\n"," Name: description, Length: 1000, dtype: object)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["class NetflixDataset(Dataset):\n","\n","    def __init__(self, txt_list, tokenizer, max_length):\n","        self.input_ids = []\n","        self.attn_masks = []\n","        self.labels = []\n","\n","        for txt in txt_list:\n","            # Encode the descriptions using the GPT-Neo tokenizer\n","            encodings_dict = tokenizer(\n","                '<|startoftext|>' + txt + '<|endoftext|>',\n","                truncation=True,\n","                max_length=max_length, \n","                padding='max_length'\n","            )\n","            \n","            input_ids = torch.tensor(encodings_dict['input_ids'])    \n","            self.input_ids.append(input_ids)\n","\n","            mask = torch.tensor(encodings_dict['attention_mask'])\n","            self.attn_masks.append(mask)\n","\n","            # label = [tokenizer.decode(x).strip() for x in encodings_dict['input_ids']]\n","            # self.labels.append(label)\n","\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx]\n","\n","dataset = NetflixDataset(descriptions, tokenizer, max_length)\n","\n","print(len(dataset))\n","\n","train_size = int(0.8 * len(dataset))\n","train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n","\n","print('train_dataset', len(train_dataset))\n","print('val_dataset', len(val_dataset))\n","train_dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6srJg0T_tjEK","executionInfo":{"status":"ok","timestamp":1665260786494,"user_tz":180,"elapsed":370,"user":{"displayName":"Matheus Vanzan","userId":"00967685822501260988"}},"outputId":"78cb6154-8942-4e1b-ef20-915b84c6994d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1000\n","train_dataset 800\n","val_dataset 200\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([50257,  5886,  6551, 12622, 30768,   422,   262,   995,  1642, 38748,\n","           292,   287,   465,  2802,   338, 46400, 15647,  1566,   257,   649,\n","         44511, 20385,   287,   290, 10114,    82,   465, 15133,    13, 50256,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n","         50258, 50258, 50258, 50258, 50258, 50258]),\n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]))"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["! pip install -q datasets evaluate"],"metadata":{"id":"KkQ61tJSYLj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here I will pass the output directory where \n","# the model predictions and checkpoints will be stored, \n","# batch sizes for the training and validation steps, \n","# and warmup_steps to gradually increase the learning rate\n","training_args = TrainingArguments(\n","    output_dir=os.path.join(PATH_DATA, model_name, 'partial'),\n","    logging_dir=os.path.join(PATH_DATA, model_name, 'logs'),\n","\n","    logging_steps=5000,\n","    save_steps=5000,  \n","\n","    num_train_epochs=5,\n","                                     \n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","\n","    warmup_steps=0,\n","    weight_decay=0.01,\n","\n","    evaluation_strategy='epoch', # -> calls on_epoch_end()\n",")\n","\n","from transformers import TrainerCallback\n","from copy import deepcopy\n","\n","# https://huggingface.co/transformers/v4.0.1/_modules/transformers/trainer_callback.html\n","\n","class CustomCallback(TrainerCallback):\n","    \n","    def __init__(self, trainer) -> None:\n","        super().__init__()\n","        self._trainer = trainer\n","    \n","    def on_epoch_end(self, args, state, control, **kwargs):\n","        print('\\n---')\n","        print('on_epoch_end()')\n","        print('state', state)\n","        print('control', control)\n","        print('')\n","        current_epoch = state.epoch\n","        previous_epoch = current_epoch -1\n","\n","\n","        # print('control.should_evaluate', control.should_evaluate)\n","        # if control.should_evaluate:\n","        #     control_copy = deepcopy(control)\n","        #     train = self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix='train')\n","        #     print('  train', train)\n","        #     eval = self._trainer.evaluate() #eval_dataset=self._trainer.val_dataset, metric_key_prefix='eval')\n","        #     print('  eval', eval)\n","        #     return control_copy\n","\n","import numpy as np\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","def compute_metrics(p):\n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred)\n","    precision = precision_score(y_true=labels, y_pred=pred)\n","    f1 = f1_score(y_true=labels, y_pred=pred)\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","trainer = Trainer(\n","    model=model, \n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset, \n","    # This custom collate function is necessary \n","    # to built batches of data\n","    data_collator=lambda data: {\n","        'input_ids': torch.stack([f[0] for f in data]),       \n","        'attention_mask': torch.stack([f[1] for f in data]),\n","        'labels': torch.stack([f[0] for f in data])\n","    },\n","    # compute_metrics=compute_metrics,\n",")\n","# Start training process!\n","# trainer.add_callback(CustomCallback(trainer)) \n","train = trainer.train()\n","trainer.evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":782},"id":"P4D0sqocuWAv","executionInfo":{"status":"ok","timestamp":1665261039675,"user_tz":180,"elapsed":55898,"user":{"displayName":"Matheus Vanzan","userId":"00967685822501260988"}},"outputId":"c982e101-1b6f-4213-b64d-d8f430413db8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running training *****\n","  Num examples = 800\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 300\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [300/300 00:54, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.068487</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>2.091509</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>2.120424</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 200\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 200\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 200\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 200\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25/25 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 2.120424270629883,\n"," 'eval_runtime': 1.0222,\n"," 'eval_samples_per_second': 195.658,\n"," 'eval_steps_per_second': 24.457,\n"," 'epoch': 3.0}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# train.metrics\n","# trainer.predict(test_dataset=val_dataset, metric_key_prefix=\"val\")"],"metadata":{"id":"QAT-PI_ox33H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trainer.save_model(os.path.join(PATH_DATA, model_name, 'model-trained'))"],"metadata":{"id":"HBw-SZNO_t05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Start every description with a special BOS token\n","# generated = tokenizer('<|startoftext|>', return_tensors='pt').input_ids.cuda()\n","\n","# Start every description with a special BOS token\n","# prompt = 'mov ebp var_A dx' # mov edx ebp var_E'\n","# prompt = 'The key benefits of TF-IDF are'\n","prompt = 'Four women'\n","generated = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n","\n","# Generate 3 movie descriptions\n","sample_outputs = model.generate(\n","    generated, \n","    # Use sampling instead of greedy decoding \n","    do_sample=True, \n","    # Keep only top 50 token with the highest probability\n","    top_k=50, \n","    # Maximum sequence length\n","    max_length=50,\n","    #max_new_tokens=100,\n","    # Keep only the most probable tokens with cumulative probability of 95%\n","    top_p=0.95, \n","    # Changes randomness of generated sequences to 1.9\n","    temperature=1,\n","    # Number of sequences to generate                 \n","    num_return_sequences=5\n",")\n","\n","# Print generated descriptions\n","for i, sample_output in enumerate(sample_outputs): \n","    print('{}: {}'.format(i, tokenizer.decode(sample_output, skip_special_tokens=True)).replace('\\n', ' '))\n","\n","# completion = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n","# print(completion)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-zzmIpOkZjg","executionInfo":{"status":"ok","timestamp":1665261040584,"user_tz":180,"elapsed":941,"user":{"displayName":"Matheus Vanzan","userId":"00967685822501260988"}},"outputId":"4b3cc6bf-ca6a-4cf8-c1a7-de364eaa1ef8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["0: Four women and children live in the city of Istanbul, where their families face challenges facing unemployment, poverty and political oppression.\n","1: Four women who slept with their boyfriends over the last five years have accused their ex-lover of cheating on them.\n","2: Four women and their families are denied a fair shake during a visit by the founder and the director of a prestigious private school, who leads them to believe that their children's magical abilities might be a gift from God.\n","3: Four women go on the run from a murderous mob with illegal weapons in Nigeria after fleeing in the face of an Indian mob, only to find that there's an even bigger threat.\n","4: Four women on Tinder have been caught cheating on men and taking up too much space on the dating scene, according to a new survey.\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"by_LO2WZDkHn","executionInfo":{"status":"ok","timestamp":1665261040585,"user_tz":180,"elapsed":20,"user":{"displayName":"Matheus Vanzan","userId":"00967685822501260988"}},"outputId":"0e3999f9-88e5-4f2a-9fa3-acb534c11ec1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Oct  8 20:30:38 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   68C    P0    40W /  70W |  10040MiB / 15109MiB |     25%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]}]}