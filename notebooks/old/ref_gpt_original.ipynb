{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"DIjF95a6aQ5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665091024846,"user_tz":180,"elapsed":64349,"user":{"displayName":"Cap Vanzan","userId":"05248490625695273722"}},"outputId":"0595051d-3ea6-4e8f-8e9f-87694d30c6db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 143 kB 28.8 MB/s \n","\u001b[K     |████████████████████████████████| 55.9 MB 290 kB/s \n","\u001b[K     |████████████████████████████████| 717 kB 71.4 MB/s \n","\u001b[K     |████████████████████████████████| 163 kB 52.8 MB/s \n","\u001b[K     |████████████████████████████████| 7.6 MB 61.8 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 2.5 MB/s \n","\u001b[K     |████████████████████████████████| 120 kB 57.4 MB/s \n","\u001b[K     |████████████████████████████████| 99 kB 10.4 MB/s \n","\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["####################################\n","#\n","#  ADD THIS TO EVERY COLAB FILE!\n","#\n","####################################\n","\n","#!pip install -q import-ipynb\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#import import_ipynb\n","import drive.Shareddrives.GPTJ.project.settings as settings\n","\n","PATH_PROJECT = settings.PATH_PROJECT\n","PATH_DATA = settings.PATH_DATA\n","\n","! cd $PATH_PROJECT && pip install -q -r requirements.txt"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4i37t3qjfJG","executionInfo":{"status":"ok","timestamp":1665091024847,"user_tz":180,"elapsed":17,"user":{"displayName":"Cap Vanzan","userId":"05248490625695273722"}},"outputId":"7cbc5250-880a-4484-f5f9-1b34b471e689"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Oct  6 21:17:02 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, random_split\n","from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel, GPTNeoForCausalLM"],"metadata":{"id":"0Rl0u8CQj0SI","executionInfo":{"status":"ok","timestamp":1665091040633,"user_tz":180,"elapsed":15798,"user":{"displayName":"Cap Vanzan","userId":"05248490625695273722"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# MODEL\n","\n","#model_name = 'EleutherAI/gpt-neo-1.3B' # CUDA out of memory.\n","\n","model_name = 'gpt2' # GPU vRAM 3495MiB - 3.5G\n","#model_name = 'gpt2-medium' # GPU vRAM 7389MiB - 7.3G\n","#model_name = 'gpt2-large' # GPU vRAM 14797MiB - 14.8G\n","\n","#model_name = 'gpt2-xl' # CUDA out of memory.\n"],"metadata":{"id":"axoAqEwsBP-0","executionInfo":{"status":"ok","timestamp":1665091040635,"user_tz":180,"elapsed":53,"user":{"displayName":"Cap Vanzan","userId":"05248490625695273722"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n","\n","#torch.save(model, os.path.join(PATH_DATA, model_name, 'model'))"],"metadata":{"id":"AR0j06rPj6PU","executionInfo":{"status":"ok","timestamp":1665091176020,"user_tz":180,"elapsed":4749,"user":{"displayName":"Cap Vanzan","userId":"05248490625695273722"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Set the random seed to a fixed value to get reproducible results \n","torch.manual_seed(42)\n","\n","prompt = 'Four women'\n","generated = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n","\n","# Generate \n","sample_outputs = model.generate(generated, \n","    # Use sampling instead of greedy decoding \n","    do_sample=True, \n","    # Keep only top 50 token with the highest probability\n","    top_k=50, \n","    # Maximum sequence length\n","    max_length=50,\n","    #max_new_tokens=100,\n","    # Keep only the most probable tokens with cumulative probability of 95%\n","    top_p=0.95, \n","    # Changes randomness of generated sequences to 1.9\n","    temperature=0.0001,\n","    # Number of sequences to generate                 \n","    num_return_sequences=3\n",")\n","\n","# Print generated descriptions\n","for i, sample_output in enumerate(sample_outputs): \n","    print('{}: {}'.format(i, tokenizer.decode(sample_output, skip_special_tokens=True)).replace('\\n', ' '))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-zzmIpOkZjg","executionInfo":{"status":"ok","timestamp":1665091190274,"user_tz":180,"elapsed":2113,"user":{"displayName":"Cap Vanzan","userId":"05248490625695273722"}},"outputId":"07d007d6-5dc4-4b80-be5e-e3bcc7c129d2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["0: Four women have been arrested in connection with the attack, which left at least seven dead and more than 100 injured.  The attack comes as the government is facing criticism for failing to tackle the problem of sexual violence in the country.  The\n","1: Four women have been arrested in connection with the attack, which left at least seven dead and more than 100 injured.  The attack comes as the government is facing criticism for failing to tackle the problem of sexual violence in the country.  The\n","2: Four women have been arrested in connection with the attack, which left at least seven dead and more than 100 injured.  The attack comes as the government is facing criticism for failing to tackle the problem of sexual violence in the country.  The\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"by_LO2WZDkHn","executionInfo":{"status":"ok","timestamp":1665091072116,"user_tz":180,"elapsed":30,"user":{"displayName":"Cap Vanzan","userId":"05248490625695273722"}},"outputId":"05228506-7259-4022-864a-1561e587692e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Oct  6 21:17:49 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    34W /  70W |   1666MiB / 15109MiB |     44%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]}]}